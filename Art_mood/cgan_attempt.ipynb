{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Style</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>arrogance</th>\n",
       "      <th>disagreeableness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>happiness</th>\n",
       "      <th>humility</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>regret</th>\n",
       "      <th>sadness</th>\n",
       "      <th>shame</th>\n",
       "      <th>shyness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>neutral</th>\n",
       "      <th>Image Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58c6237dedc2c9c7dc0de1ae</td>\n",
       "      <td>Modern Art</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resized_images\\1_resized.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>577280dfedc2cb3880f28e76</td>\n",
       "      <td>Modern Art</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resized_images\\2_resized.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57727f2dedc2cb3880ed5fa9</td>\n",
       "      <td>Modern Art</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resized_images\\3_resized.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58d1240cedc2c94f900fc610</td>\n",
       "      <td>Modern Art</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resized_images\\4_resized.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57727de7edc2cb3880e91f26</td>\n",
       "      <td>Post Renaissance Art</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>resized_images\\5_resized.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID                 Style  agreeableness  anger  \\\n",
       "0  58c6237dedc2c9c7dc0de1ae            Modern Art          0.060  0.012   \n",
       "1  577280dfedc2cb3880f28e76            Modern Art          0.000  0.000   \n",
       "2  57727f2dedc2cb3880ed5fa9            Modern Art          0.000  0.000   \n",
       "3  58d1240cedc2c94f900fc610            Modern Art          0.000  0.000   \n",
       "4  57727de7edc2cb3880e91f26  Post Renaissance Art          0.077  0.077   \n",
       "\n",
       "   anticipation  arrogance  disagreeableness  disgust   fear  gratitude  \\\n",
       "0         0.071      0.024             0.012    0.000  0.012      0.119   \n",
       "1         0.100      0.000             0.000    0.100  0.300      0.000   \n",
       "2         0.200      0.000             0.000    0.000  0.000      0.100   \n",
       "3         0.091      0.000             0.000    0.091  0.000      0.000   \n",
       "4         0.077      0.231             0.308    0.308  0.308      0.231   \n",
       "\n",
       "   happiness  humility   love  optimism  pessimism  regret  sadness  shame  \\\n",
       "0      0.726     0.369  0.250     0.274      0.012   0.000    0.131  0.000   \n",
       "1      0.100     0.100  0.000     0.200      0.200   0.100    0.200  0.000   \n",
       "2      0.500     0.300  0.000     0.300      0.000   0.000    0.000  0.000   \n",
       "3      0.091     0.091  0.000     0.000      0.182   0.000    0.364  0.000   \n",
       "4      0.154     0.077  0.077     0.231      0.231   0.077    0.231  0.154   \n",
       "\n",
       "   shyness  surprise  trust  neutral                    Image Path  \n",
       "0    0.024     0.024  0.250      0.0  resized_images\\1_resized.png  \n",
       "1    0.000     0.500  0.000      0.0  resized_images\\2_resized.png  \n",
       "2    0.000     0.100  0.200      0.0  resized_images\\3_resized.png  \n",
       "3    0.000     0.273  0.000      0.0  resized_images\\4_resized.png  \n",
       "4    0.077     0.154  0.231      0.0  resized_images\\5_resized.png  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pre = pd.read_csv('updated_dataframe.csv')\n",
    "df_pre = df_pre.drop(columns=['Image URL'])\n",
    "pd.options.display.max_columns = None\n",
    "display(df_pre.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"model_7\" (type Functional).\n\nInput 0 of layer \"dense_18\" is incompatible with the layer: expected axis -1 of input shape to have value 2880022, but received input with shape (None, 2582)\n\nCall arguments received by layer \"model_7\" (type Functional):\n  • inputs=['tf.Tensor(shape=(None, 17, 16, 3), dtype=float32)', 'tf.Tensor(shape=(None, 22), dtype=float32)']\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Joel\\Documents\\Github\\MLA\\Art_mood\\cgan_attempt.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W1sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m label \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(num_classes,))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W1sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m fake_image \u001b[39m=\u001b[39m generator([noise, label])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W1sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m validity \u001b[39m=\u001b[39m discriminator([fake_image, label])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W1sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m cgan \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mModel([noise, label], validity)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W1sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m cgan\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0002\u001b[39m, beta_1\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Joel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Joel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py:280\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    275\u001b[0m             value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mvalue\n\u001b[0;32m    276\u001b[0m         \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shape_as_list[\u001b[39mint\u001b[39m(axis)] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\n\u001b[0;32m    277\u001b[0m             value,\n\u001b[0;32m    278\u001b[0m             \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         }:\n\u001b[1;32m--> 280\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    282\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: expected axis \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof input shape to have value \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mbut received input with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape \u001b[39m\u001b[39m{\u001b[39;00mdisplay_shape(x\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m             )\n\u001b[0;32m    287\u001b[0m \u001b[39m# Check shape.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mshape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m shape\u001b[39m.\u001b[39mrank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"model_7\" (type Functional).\n\nInput 0 of layer \"dense_18\" is incompatible with the layer: expected axis -1 of input shape to have value 2880022, but received input with shape (None, 2582)\n\nCall arguments received by layer \"model_7\" (type Functional):\n  • inputs=['tf.Tensor(shape=(None, 17, 16, 3), dtype=float32)', 'tf.Tensor(shape=(None, 22), dtype=float32)']\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Data preprocessing function for images\n",
    "def preprocess_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (600, 600))\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    return image\n",
    "\n",
    "# Create an ImageDataGenerator for real images\n",
    "real_image_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    "    rescale=1./127.5 - 1  # Scale to the range [-1, 1]\n",
    ")\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_df, test_df = train_test_split(df_pre, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator(latent_dim, num_classes):\n",
    "    noise = layers.Input(shape=(latent_dim,))\n",
    "    label = layers.Input(shape=(num_classes,))\n",
    "    \n",
    "    x = layers.Concatenate()([noise, label])\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Reshape((4, 4, 16))(x)\n",
    "    \n",
    "    # Upsample to (600, 600, 3)\n",
    "    x = layers.Conv2DTranspose(256, (4, 4), strides=(4, 4), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(3, (3, 3), padding='same', activation='tanh')(x)\n",
    "    \n",
    "    generated_image = layers.Reshape((600, 600, 3))(x)\n",
    "    \n",
    "    return keras.Model([noise, label], generated_image)\n",
    "\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_generator(latent_dim, num_classes):\n",
    "    noise = layers.Input(shape=(latent_dim,))\n",
    "    label = layers.Input(shape=(num_classes,))\n",
    "    \n",
    "    x = layers.Concatenate()([noise, label])\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Reshape((4, 4, 16))(x)\n",
    "    x = layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Add ZeroPadding2D to adjust the dimensions\n",
    "    x = layers.ZeroPadding2D(padding=((1, 0), (0, 0)))(x)\n",
    "    \n",
    "    generated_image = layers.Conv2D(3, (4, 4), padding='same', activation='tanh')(x)\n",
    "    \n",
    "    return keras.Model([noise, label], generated_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build the CGAN\n",
    "latent_dim = 100\n",
    "img_shape = (600, 600, 3)\n",
    "num_classes = len(df_pre.columns) - 1\n",
    "\n",
    "discriminator = build_discriminator(img_shape, num_classes)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(latent_dim, num_classes)\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "noise = layers.Input(shape=(latent_dim,))\n",
    "label = layers.Input(shape=(num_classes,))\n",
    "fake_image = generator([noise, label])\n",
    "validity = discriminator([fake_image, label])\n",
    "\n",
    "cgan = keras.Model([noise, label], validity)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "# Training loop\n",
    "batch_size = 64\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(train_df.shape[0] // batch_size):\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        labels = train_df.drop(columns='Image Path').sample(batch_size, axis=0).to_numpy()\n",
    "\n",
    "        # Generate fake images using the generator\n",
    "        fake_images = generator.predict([noise, labels])\n",
    "\n",
    "        # Load a batch of real images and labels\n",
    "        real_image_paths = train_df['Image Path'].sample(batch_size, replace=True).tolist()\n",
    "        real_images = []\n",
    "\n",
    "        for image_path in real_image_paths:\n",
    "            image = Image.open(image_path)\n",
    "            image = image.resize((600, 600))\n",
    "            image = np.array(image)  # Convert to NumPy array\n",
    "            image = (image.astype(np.float32) - 127.5) / 127.5  # Scale to range [-1, 1]\n",
    "            real_images.append(image)\n",
    "\n",
    "        real_images = np.array(real_images)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch([real_images, labels], np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch([fake_images, labels], np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = cgan.train_on_batch([noise, labels], np.ones((batch_size, 1)))\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape_15\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [22, 360000], output_shape = [600, 600, 1]\n\nCall arguments received by layer \"reshape_15\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 22, 360000), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Joel\\Documents\\Github\\MLA\\Art_mood\\cgan_attempt.ipynb Cell 3\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m img_shape \u001b[39m=\u001b[39m (\u001b[39m600\u001b[39m, \u001b[39m600\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m num_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df_pre\u001b[39m.\u001b[39mcolumns) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m discriminator \u001b[39m=\u001b[39m build_discriminator(img_shape, num_classes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m discriminator\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0002\u001b[39m, beta_1\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m generator \u001b[39m=\u001b[39m build_generator(latent_dim, num_classes)\n",
      "\u001b[1;32mc:\\Users\\Joel\\Documents\\Github\\MLA\\Art_mood\\cgan_attempt.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Embed the label information\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m label_embedding \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mEmbedding(num_classes, img_shape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m img_shape[\u001b[39m1\u001b[39m])(label)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m label_embedding \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mReshape((img_shape[\u001b[39m0\u001b[39;49m], img_shape[\u001b[39m1\u001b[39;49m], \u001b[39m1\u001b[39;49m))(label_embedding)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConcatenate()([image, label_embedding])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Joel/Documents/Github/MLA/Art_mood/cgan_attempt.ipynb#W2sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(\u001b[39m64\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), strides\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(x)\n",
      "File \u001b[1;32mc:\\Users\\Joel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Joel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[1;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[0;32m    116\u001b[0m     output_shape[unknown] \u001b[39m=\u001b[39m original \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m known\n\u001b[0;32m    117\u001b[0m \u001b[39melif\u001b[39;00m original \u001b[39m!=\u001b[39m known:\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m output_shape\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_15\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [22, 360000], output_shape = [600, 600, 1]\n\nCall arguments received by layer \"reshape_15\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 22, 360000), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_df, test_df = train_test_split(df_pre, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator(latent_dim, num_classes):\n",
    "    noise = layers.Input(shape=(latent_dim,))\n",
    "    label = layers.Input(shape=(num_classes,))\n",
    "    \n",
    "    x = layers.Concatenate()([noise, label])\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Reshape((4, 4, 16))(x)\n",
    "    \n",
    "    # Upsample to (600, 600, 3)\n",
    "    x = layers.Conv2DTranspose(256, (4, 4), strides=(4, 4), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Add ZeroPadding2D to adjust the dimensions\n",
    "    x = layers.ZeroPadding2D(padding=((1, 0), (0, 0)))(x)\n",
    "    \n",
    "    generated_image = layers.Conv2D(3, (3, 3), padding='same', activation='tanh')(x)\n",
    "    \n",
    "    return keras.Model([noise, label], generated_image)\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator(img_shape, num_classes):\n",
    "    image = layers.Input(shape=img_shape)\n",
    "    label = layers.Input(shape=(num_classes,))\n",
    "    \n",
    "    # Embed the label information\n",
    "    label_embedding = layers.Embedding(num_classes, img_shape[0] * img_shape[1])(label)\n",
    "    label_embedding = layers.Reshape((img_shape[0], img_shape[1], 1))(label_embedding)\n",
    "    \n",
    "    x = layers.Concatenate()([image, label_embedding])\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return keras.Model([image, label], x)\n",
    "\n",
    "# Build the CGAN\n",
    "latent_dim = 100\n",
    "img_shape = (600, 600, 3)\n",
    "num_classes = len(df_pre.columns) - 1\n",
    "\n",
    "discriminator = build_discriminator(img_shape, num_classes)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator(latent_dim, num_classes)\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "noise = layers.Input(shape=(latent_dim,))\n",
    "label = layers.Input(shape=(num_classes,))\n",
    "fake_image = generator([noise, label])\n",
    "validity = discriminator([fake_image, label])\n",
    "\n",
    "cgan = keras.Model([noise, label], validity)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "\n",
    "# Training loop (adjust this according to your data and needs)\n",
    "batch_size = 64\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(train_df.shape[0] // batch_size):\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        labels = train_df.drop(columns='Image Path').sample(batch_size, axis=0).to_numpy()\n",
    "        \n",
    "       # Generate fake images using the generator\n",
    "        fake_images = generator.predict([noise, labels])\n",
    "\n",
    "        # Load a batch of real images and labels\n",
    "        real_image_paths = train_df['Image Path'].sample(batch_size, replace=True).tolist()\n",
    "        real_images = []\n",
    "\n",
    "        for image_path in real_image_paths:\n",
    "            image = Image.open(image_path)\n",
    "            image = image.resize((600, 600))\n",
    "            image = np.array(image)  # Convert to NumPy array\n",
    "            image = (image.astype(np.float32) - 127.5) / 127.5  # Scale to range [-1, 1]\n",
    "            real_images.append(image)\n",
    "\n",
    "        real_images = np.array(real_images)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch([real_images, labels], np.ones((batch_size, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch([fake_images, labels], np.zeros((batch_size, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = cgan.train_on_batch([noise, labels], np.ones((batch_size, 1)))\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
